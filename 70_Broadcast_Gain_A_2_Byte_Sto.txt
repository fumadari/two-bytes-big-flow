Broadcast-Gain: A 2-Byte, Stop-Gradient Control Plane to Trim
Long-Tail Latency in Cooperative MARL

Anonymous Authors

Abstract

Cooperative Multi-Agent Reinforcement Learning (MARL) over bursty, lossy links faces delayed/sparse rewards, high-
variance gradients, and learned communication that assumes smooth channels. We introduce Broadcast–Gain (BG), a
fixed-rate, 2-byte, stop-gradient neighbor broadcast that overlays a standard PPO+GAE policy with no changes to train-
ing or rewards. Each cycle, an agent sends one byte encoding a residual of local pressure-progress and one byte of coarse
context (axis bit, distance bin). Receivers keep the freshest packets and form a confidence-weighted consensus that gates
a simple phase scheduler; the overlay only nudges the MOVE logit via a tiny multiplier and a narrow, distance-decayed
push near the gate. Bandwidth is ∼ 0.24 kbit/s per agent; compute is a few scalar ops per step.

We evaluate a single-junction grid with a c-step clearance lock across N ∈ {100, 120, 140}, per-tick packet drop probabil-
ities {0.60, 0.65, 0.70}, and cycle len ∈ {3, 5, 6}. For each cell we compare a frozen baseline to the same frozen policy
with BG (constants fixed). BG trims tails where it matters: on the hardest cell (N =120, drop 0.70, 6-step cycle) p95 wait
(95th-percentile steps-to-clear) drops by 4.97 steps and near-gate flow rises by +392/1k, with idle-red ≈ 0. Across 108
cells BG wins 78 (72%), with gains concentrated at larger N and longer cycles and graceful degradation as drops increase.
Mechanism checks show reallocation into green (+16–20 pp) and higher near-gate flow, consistent with a consensus gate
that stretches minimum green under weak information and flips knife-edge outcomes without thrash.

BG is neighbor-only, event-based, robust to drops, and drops in without touching the learner.

Keywords:

reinforcement

multi-agent
long-horizon control;
bandwidth-efficient communication; stop-gradient coordination;
neuromodulatory gain

learning;

1 Introduction

Cooperative MARL over long horizons breaks when bandwidth is scarce and delivery is bursty. Delayed, sparse re-
wards raise gradient variance; learned communication often assumes rich, differentiable channels [1–4]; and centralized
critics or value factorization stabilize training only with wide access and heavier models—poor fits when agents get a
few bits per tick and links drop packets [5–8]. Bandwidth-aware schedulers and information-efficiency methods adapt
what/when to talk but add complexity and training burden [9, 10]. The gap is a tiny, robust control-plane signal that
works under packet loss and stays compatible with standard policy learning.

We propose Broadcast–Gain (BG): a fixed-rate, two-byte, stop-gradient broadcast that supplies a small, confidence-
weighted global cue and a targeted push near the junction. It is neighbor-only, requires no learned protocol or back-
propagation through the channel, and overlays a standard PPO+GAE policy [11, 12]. In short, BG trades rich messages
and attention for a minimal cue that gates phase by consensus and lengthens minimum green when information is weak.

Despite its size, BG moves the needle. On a hard evaluation cell (N =120, dropout 0.70, cycle len=6), it reduces the 95th-
percentile wait by 4.97 steps and adds +392 near-gate crossings per 1k steps, with idle-red ≈ 0, at ∼0.24 kbit/s per agent.
Across settings, gains concentrate where tails are largest and degrade gracefully as loss increases. Our contributions
are a two-byte stop-gradient broadcast primitive, a confidence-aware gate that tolerates loss, and evidence that such a
minimal overlay reliably trims long-tail latency without changing the base learner.

2 Method: Broadcast–Gain

Broadcast–Gain is a stop-gradient overlay on a standard policy. It adds a fixed neighbor broadcast each cycle, fuses
received hints into a single confidence-weighted cue, drives a phase scheduler for the junction, and applies a near-gate
push that adjusts the move logit.

Setting. Two perpendicular corridors share one junction with a c-step clearance lock (Fig. 1). Agents act every step with
local observations. Communication is neighbor-only and fixed-rate (once per cycle). The junction exposes a served axis
S ∈ {+1, −1} that may switch at cycle boundaries.

Once per cycle, each agent i broadcasts two bytes: (1) a one-byte residual zi summarizing local progress/pressure (int8;
optional µ-law), and (2) a one-byte meta tag (axis bit, distance bin). Messages are sent within a small Manhattan radius;
receivers keep the freshest packet per sender under a short TTL. Unique senders are aggregated into per-axis estimates.
A moving average of coverage/freshness yields an information weight winfo ∈ [0, 1], and a simple consensus score rises
when most senders favor the same axis. These combine into a gate wcons = gate(consensus, winfo) ∈ [0, 1], which increases
with agreement and coverage and decays smoothly as packets are lost.

The scheduler maintains S. Each cycle it enforces a minimum green that stretches when winfo is low, then switches when
an advantage built from the fused signals clears a confidence-scaled threshold or when a max-green limit hits. If a cycle
was wasted-clear (lock held, no crossing), the next minimum green is shortened to damp oscillations. This procedure is
local and carries no gradients. The overlay touches only the move logit:

move = g(i)
y(i)

mul ℓ(i)

move + g(i)
add,

move = σ(cid:0)y(i)
p(i)

move

(cid:1),

with a tiny multiplicative term g(i)

mul ≈ 1 and a near-gate additive push

(cid:16)
g(i)
add = clip

Λ sgn(siS) e−di/τ wcons + γfair wcons ϕi, −A, A

(cid:17)

.

Here si ∈ {+1, −1} is the agent’s axis, di its grid distance to the gate, and ϕi a green-only fairness term that grows
with near-gate wait. Green receives a small positive push; red a soft brake, with a hard-stop band for di ≤ dstop. A
short open window just past the gate enables platooning. Bandwidth is fixed at two bytes per agent per cycle (e.g.,
2 B × 15 Hz × 8 = 240 bps ≈ 0.24 kbit/s with 60 Hz and C=4).

Small-perturbation guarantee. Let |δ| ≤ A denote the total clipped shift BG applies to the MOVE logit at a state (we fold
the tiny multiplicative term into δ via its effect on the logit and clip). Then the overlay changes the policy only a little:
Theorem 1 (Tight drift for a single-logit push). For any observation o, if πBG is obtained from π by shifting only the MOVE logit
by δ (others unchanged), then

DTV

(cid:0)πBG(·|o), π(·|o)(cid:1) = (cid:12)

(cid:12)πBG(MOVE|o) − π(MOVE|o)(cid:12)

(cid:12) ≤ tanh

(cid:17)

,

(cid:16) |δ|
4

and

DKL

(cid:0)πBG(·|o) ∥ π(·|o)(cid:1) ≤

DKL

(cid:0)π(·|o) ∥ πBG(·|o)(cid:1) ≤

δ2
8

.

δ2
8

,

1

Eval: N =120, dropout =0.70, cycle len =6 (frozen → BG), 3 seeds

Variant (eval)

SII Tail p95 ∆ ↓ (steps) Near-gate ∆ ↑ (/1k)

Frozen baseline (ref.) 0.000
0.450
BG (TD)
0.240
BG (RawEnt)

0.00
-4.97
-2.35

0.0
+391.9
+328.5

Mechanism (train): N =140, dropout =0.70, cycle len =6 - BG (TD)

Variant ∆share att green (pp) ∆share real green (pp) ∆near-gate

BG (TD)

+16.17

+19.89

+160.6

Table 1: Broadcast–Gain (BG) results. Right, top: strongest eval cell
(N =120, dropout =0.70, cycle len =6), comparing a frozen baseline to
the same policy with BG. Right, bottom: mechanism check on the train
run cell (N =140, dropout =0.70, cycle len =6).

Figure 1: Experimental setup visualization.

3 Experiments

Setup and metrics. Single-junction grid with a c-step clearance lock. Factors: N ∈ {100, 120, 140}, per-tick packet
dropout {0.60, 0.65, 0.70}, and cycle len ∈ {3, 5, 6}. For each cell we run matched seeds and compare a frozen
PPO+GAE policy to the same frozen policy with the BG overlay; BG constants are fixed across cells (no per-cell tun-
ing). The primary metric is tail wait p95 (↓). Secondaries are near-gate realized crossings (per 1k steps, ↑), idle-red (↓),
and (train-only) gate efficiency (↑) used for mechanism checks. For ranking only, we report a Signed Improvement Index
(SII): a signed z-score combining (−p95, +near-gate) relative to the frozen baseline (SII> 0 favors BG).

Results. Reference stress cell (N =120, dropout 0.70, cycle len=6): BG(TD) reduces p95 by 4.97 steps and increases
near-gate crossings by 391.9 per 1k steps, with idle-red ≈ 0 (SII = 0.450). The RawEnt variant yields 2.35 and +328.5,
respectively. These shifts trim the tail without inducing red-time idling, consistent with a targeted near-gate push (Ta-
ble. 1).

Across cells, BG wins 78/108 (72%). Gains concentrate at longer cycles and larger N ; very short cycles (3) can be neutral
or negative. Typical near-gate improvements are +249–+354 per 1k steps. We observe a small dip in direction-normalized
gate efficiency (mean ≈ −0.02). Better tails correlate with this dip (Spearman rs ≈ 0.53; scatter in the appendix), consistent
with reallocating green time where it matters.

Mechanism checks and ablations (train reference case: N =140, 0.70, 6): BG(TD) shifts attention-green by +16.17 pp, realized-
green by +19.89 pp, and near-gate by +160.6 per 1k. Removing the near-gate push, removing confidence, or compressing
to one byte each weakens or eliminates these gains (appendix).

4 Conclusion

Broadcast–Gain is a two-byte, stop-gradient control-plane overlay that reduces long-tail latency under bursty delivery
at negligible cost (0.24 kbit/s per agent and a few scalar ops per step). In the most demanding evaluation case (N =120,
dropout =0.70, cycle len=6), BG lowers tail p95 by 4.97 steps and increases near-gate crossings by 392 per 1k steps
while keeping idle-red near zero. Gains are strongest at longer cycles and larger N , and the method degrades gracefully
as packet loss increases; very short cycles can be neutral or slightly negative.

Mechanistically, BG supplies a small, reliable global cue without learning through the channel: neighbors form a
confidence-weighted consensus that gates phase decisions; a narrow near-gate push resolves knife-edge conflicts; and a
light damping term reduces wasted clear. This recovers much of the effect of max-pressure with microscopic bandwidth
and without altering the underlying PPO policy or rewards.

The approach aligns with event-driven, local-to-global coordination trends (e.g., robot-centric and graph-floor models
with asynchronous updates). A pragmatic integration is to pair a learned short-horizon predictor with a 2-byte BG gate
at execution time, keeping learning off the link while remaining robust to bursty loss [13].

Looking ahead, the most impactful extensions are: adaptive rate/quantization and TTL driven by uncertainty; forecast-
to-gate fusion that modulates the consensus weight and near-gate strength; generalization to merges, splits, and multi-
phase controllers; multi-hop sparse consensus for larger floors; and sim-to-real studies on MAPF-like layouts with con-
gested Wi-Fi. Our view is that tiny, stop-gradient broadcasts are an underused lever in long-horizon MARL - practical to
deploy, robust under adversity, and complementary to richer learned predictors rather than competing with them.

2

References

[1] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with backpropagation. In Advances

in Neural Information Processing Systems (NeurIPS), 2016. URL https://arxiv.org/abs/1605.07736.

[2] Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson.

Learning to communicate with
In Advances in Neural Information Processing Systems (NeurIPS), 2016. URL

deep multi-agent reinforcement learning.
https://arxiv.org/abs/1605.06676.

[3] Abhishek Das, Th´eophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Michael Rabbat, and Joelle Pineau. Tarmac: Targeted
multi-agent communication. In Proceedings of the 36th International Conference on Machine Learning (ICML), volume 97 of PMLR,
2019. URL https://proceedings.mlr.press/v97/das19a/das19a.pdf.

[4] Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar.

In International Conference on Learning Representations (ICLR), 2019.

Learning when to communicate at scale in multia-
URL

gent cooperative and competitive tasks.
https://arxiv.org/abs/1812.09755. IC3Net.

[5] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-

competitive environments. arXiv preprint arXiv:1706.02275, 2017. URL https://arxiv.org/abs/1706.02275.

[6] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent

policy gradients. In AAAI Conference on Artificial Intelligence, 2018. URL https://arxiv.org/abs/1705.08926.

[7] Peter Sunehag, Guy Lever, Audr ¯unas Gruslys, Wojciech M. Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas
Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel. Value-decomposition networks for cooperative multi-agent learning.
arXiv preprint arXiv:1706.05296, 2017. URL https://arxiv.org/abs/1706.05296.

[8] Tabish Rashid, Mikayel Samvelyan, Christian Schr ¨oder de Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix:
Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learn-
ing (ICML), 2018. URL https://arxiv.org/abs/1803.11485.

[9] Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son, and Yung Yi. Learning to
In International Conference on Learning Representations (ICLR),

schedule communication in multi-agent reinforcement learning.
2019. URL https://arxiv.org/abs/1902.01554. SchedNet.

[10] Rui Wang, Xiaolong Guo, Chao Yu, Zhen Xiao, Changjie Fan, and Jun Wang. Learning efficient multi-agent communication: An
information-theoretic perspective. arXiv preprint arXiv:1911.06992, 2019. URL https://arxiv.org/abs/1911.06992.
[11] John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-dimensional continuous control using
generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. URL https://arxiv.org/abs/1506.02438.
[12] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv

preprint arXiv:1707.06347, 2017. URL https://arxiv.org/abs/1707.06347.

[13] Ameya Agaskar, Sriram Siva, William Pickering, Kyle O’Brien, Charles Kekeh, Ang Li, Brianna Gallo Sarker, Alicia Chua,
Mayur Nemade, Charun Thattai, Jiaming Di, Isaac Iyengar, Ramya Dharoor, Dino Kirouani, Jimmy Erskine, Tamir Hegazy,
Scott Niekum, Usman A. Khan, Federico Pecora, and Joseph W. Durham. Deepfleet: Multi-agent foundation models for mobile
robots. arXiv preprint arXiv:2508.08574, 2025. URL https://arxiv.org/abs/2508.08574.

3

A Proofs for §2: Small-perturbation guarantees

Proof of Theorem 1. Let the action set have size K ≥ 2 and let ℓ ∈ RK be baseline logits with π = softmax(ℓ). Denote the
MOVE action by m and set p = π(m|o). BG shifts only the MOVE logit: ℓ′
m = ℓm + δ, ℓ′
j = ℓj for j ̸= m, with |δ| ≤ A (after
absorbing the multiplicative term into δ and clipping as stated in the main text). Then

p′ ≜ πBG(m|o) =

eℓm+δ
eℓm+δ + (cid:80)

j̸=m eℓj

= σ(cid:0) ℓm − log

(cid:88)

j̸=m

eℓj

+δ(cid:1) = σ(θ + δ),

(cid:124)

(cid:123)(cid:122)
logit(p)

(cid:125)

where θ = logit(p) and p = σ(θ). For j ̸= m, probabilities rescale by a common factor α = 1−p′

1−p , i.e., π′( j |o) = α π(j|o).

Total variation. Because all non-MOVE coordinates scale identically,

∥π′ − π∥1 = |p′ − p| +

(cid:88)

j̸=m

|α π(j|o) − π(j|o)| = |p′ − p| + |α − 1| (1 − p) = 2|p′ − p|.

Hence DTV(π′, π) = 1
with σ′(u) = σ(u)(cid:0)1 − σ(u)(cid:1); by symmetry of σ′ about 0, g′(x) = 0 iff x = −δ/2. Evaluating,

2 ∥π′ − π∥1 = |p′ − p|. To bound |p′ − p|, define g(x) = σ(x + δ) − σ(x). Then g′(x) = σ′(x + δ) − σ′(x)

|g(x)| = (cid:12)

(cid:12)σ(δ/2) − σ(−δ/2)(cid:12)

(cid:12) = 2σ(δ/2) − 1 = tanh(δ/4),

max
x

so |p′ − p| ≤ tanh(|δ|/4). (Also |p′ − p| ≤ ∥σ′∥∞|δ| = |δ|/4 for a linear small-shift bound.)

KL bounds. Because only one logit changes and the rest redistribute proportionally, both divergences reduce to the
Bernoulli KL between (p′, 1 − p′) and (p, 1 − p). Let A(θ) = log(1 + eθ) be the Bernoulli log-partition with A′′(θ) =
σ(θ)(1 − σ(θ)) ≤ 1

4 ) and standard exponential-family identities,

4 . By L-smoothness (L = 1

and symmetrically DKL

(cid:0)Bern(σ(θ)) ∥ Bern(σ(θ + δ))(cid:1) ≤ δ2
8 .

DKL

(cid:0)Bern(σ(θ + δ)) ∥ Bern(σ(θ))(cid:1) ≤ L

2 δ2 = δ2
8 ,

Hard-stop safety. (a.k.a. Lemma A) If si ̸= S and di ≤ dstop, the overlay clamps gadd = −A ≤ 0. Since softmax is monotone
in each coordinate, decreasing the MOVE logit cannot increase its probability, i.e., πBG(MOVE|o) ≤ π(MOVE|o).

Conservative performance bound. (Corollary A) Let J(π) be the γ-discounted return and ϵ = maxs
A standard TV-based performance difference bound yields

(cid:12)Ea∼πBG(·|s)[Aπ(s, a)](cid:12)
(cid:12)
(cid:12).

J(πBG) ≥ J(π) + Es∼dπ

(cid:104) (cid:88)

πBG(a|s) Aπ(s, a)

(cid:105)

−

a

and Theorem 1 gives worst-case regret O(cid:0) tanh(A/4)(cid:1).

2γ

(1 − γ)2 ϵ Dmax

TV (πBG, π),

Absorbing the multiplicative term. (Lemma A) If the effective perturbation on the MOVE logit is y = gmulℓ + gadd with
gmul ∈ [1 − ε, 1 + ε], |gadd| ≤ A, and logits clipped |ℓ| ≤ L, then y = ℓ + δ with δ = (gmul − 1)ℓ + gadd and |δ| ≤ A + εL.
Thus Theorem 1 holds with A (cid:55)→ A + εL.

No-Zeno switching. (Lemma A) If min green eff ≥ m > 0 at every cycle boundary, then over T steps with cycle length
C, the number of flips is at most ⌈T /(C m)⌉ (each flip forces at least m full cycles of hold).

Remark. BG is a small, stop-gradient perturbation: per state DTV is at most tanh(A/4), KL drift is O(A2), hard-stop cannot
increase red encroachment, and a positive minimum green rules out pathological flip rates.

4

B Protocol, Experiments, and Robustness

Cycle and neighborhood. A cycle groups C environment steps. Each agent transmits at most once per cycle to neighbors
within Manhattan radius R; per sender, only the freshest packet is kept for up to TTTL cycles.

Two bytes. Each agent i broadcasts pkti = [ zi | mi ] ∈ {−128, . . . , 127} × {0, . . . , 255}. Byte 0 (zi): signed int8 residual
via µ-law companding with µ=255. Default (TD): with δ(i)
TD/sδ, −1, 1) and
compand

TD = r + γV (o′) − V (o), normalize x = clip(δ(i)

q = sign(x)

ln(1 + µ|x|)
ln(1 + µ)

,

zi = clip(cid:0)⌊127 q⌉, −127, 127(cid:1).

Alternative (RawEnt): x = 1 − H(π(·|oi))/Hmax, then compand/quantize as above. Byte 1 (mi): packs axis and distance,
mi = (axis bit ≪ 7) | (dist bin & 0x7F ), axis bit ∈ {0, 1} ⇔ si ∈ {−1, +1}, dist bin = min(⌊di/∆d⌋, 127).

Let Na be the set of unique fresh senders supporting axis a ∈ {−1, +1}. Decompand zj via ˆzj = sign(zj)(cid:0)(1 + µ)|zj |/127 −
1(cid:1)/µ. Weight freshness by ηj = exp(−∆tj/τfresh) and form axis scores

Za =

(cid:88)

j∈Na

ηj ˆzj,

winfo = c · meanj(ηj),

c = min

(cid:16) (cid:80)

a |Na|
Nref

(cid:17)

, 1

∈ [0, 1].

Consensus:

ρ = tanh

(cid:16) Z+1 − Z−1
κ

(cid:17)

,

wcons = σ(α ρ) winfo, σ(x) = 1

1+e−x .

Near-gate logit adjustment (per step). BG touches only the MOVE logit ℓ(i)

move = ℓ(i)
y(i)

move + g(i)
add,

p(i)
move

move:
′ = σ(cid:0)logit(p(i)

move) + g(i)

add

(cid:1),

so the change is a shift by g(i)
and fairness accumulator ϕi ∈ [0, 1],

add in the log-odds of moving. With served axis S ∈ {−1, +1}, agent axis si, grid distance di,

(cid:17)
Λ sgn(siS) e−di/τ wcons + γfair wcons ϕi, −A, A
,

(cid:16)
g(i)
add = clip
(cid:26)min(1, ϕi + 1/Kfair),

ϕi ←

max(0, ϕi − 1/Kfair), otherwise,

si=S, di ≤ dfair,

reset ϕi on crossing.

Safety: if si ̸= S and di ≤ dstop, force g(i)
enable small platoons.

add = −A (hard-stop). A short “open window” (Wopen steps) with Λopen ≤ Λ can

Scheduler (cycle boundary).

Algorithm 1: BG scheduler (compact)

return HOLD

thresh ← θ0 + θ1(1 − winfo)

1: state: S ∈ {±1}, green age, min green 0
2: inputs: Z±1, winfo, last-cycle crossings x
3: green age ← green age +1; min green eff ← min green 0 +λstretch(1 − winfo)
4: if green age < min green eff then
5:
6: end if
7: ∆Z ← Z−S − ZS;
8: if ∆Z > thresh or green age≥max green then
9:
10:
11:
12:
13:
14:
15: else
16:
17: end if

min green 0← (1 − βmg)·min green 0+βmg·min green tgt

min green 0← max(min green min, min green 0 − ∆wc)

S ← −S; green age← 0
if x=0 then

return HOLD

end if

else

5

Figure 2: TD vs. RawEnt. Signed Improvement Index (higher is better) for BG with TD (hatched) vs. RawEnt (solid)
on selected cells, ordered by ∆SII = SII(TD) − SII(RawEnt). TD dominates on heavier loads/longer cycles; RawEnt is
competitive on lighter cells.

Bitrate and compute. Per agent: 16 bits × fstep/C bps (e.g., 60 Hz and C=4 ⇒ ∼ 240 bps). Per step: O(|N |) scalar ops
(decompand, freshness, two sums) and one logit add; no extra networks.

Environment, training, evaluation. Single four-way junction with clearance lock c=C. Local observations; actions
{MOVE,WAIT}. Directed links drop with Bernoulli rate p ∈ {0.60, 0.65, 0.70}. Train PPO+GAE without BG, then freeze.
For each cell (N, dropout, C), evaluate baseline vs. BG from identical RNG snapshots and seeds {13, 17, 23}. Training
per seed: 2000 PPO updates (rollout 2048, 32 minibatches, 4 epochs), Adam lr 3×10−4, γ=0.99, λ=0.95, clip 0.2, en-
tropy/value coefs 0.01/0.5, grad-norm clip 0.5. Evaluation: 4×105 env steps/seed/cell at 60 Hz. Observations and
returns use running mean/var normalization.

Metrics and ranking. Primary: near-gate wait p95—for each crossing, count steps from first entry to d ≤ dfair until
crossing; take pooled empirical 95th percentile across matched seeds. Secondaries: near-gate crossings per 1k steps and
idle-red (fraction of steps with near-gate demand held red). For compact comparison: Signed Improvement Index (SII)
from standardized deltas: SII = 1
2

(cid:0) − z(∆p95) + z(∆NG)(cid:1).

Sensitivity and robustness (brief). Gains concentrate at larger N and longer cycles; very short cycles (C=3) can be
neutral/negative. Neighborhood and staleness matter: R ∈ [2, 3] and TTTL ∈ {1, 2} give stable wins—larger values
add coverage but inject stale/conflicting packets that raise idle-red. Gate/push should be tuned jointly: prefer rais-
ing Λ (push) before clip A; large A can induce oscillations. For C=3, increase min-green stretch and slightly reduce
switch-threshold slope to avoid premature flips under weak information. Under bursty loss (Gilbert–Elliott with mean
burst length LB and average loss ¯ϵ; transitions r=1/LB, p = r(¯ϵ − ϵG)/(ϵB − ¯ϵ)), freshness weighting and TTL degrade
gracefully with LB.

Sanity check vs. ε-max-pressure. A non-learning controller that flips when ∆Q=Q−S−QS exceeds a threshold Θ (with
tie-region randomization ε) recovers part of BG’s benefit but idles red more; BG’s consensus × confidence gate plus
near-gate push reallocates green without idling.

6

